{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission 3: Denoising Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "This DAE machine learning architecture is inspired by the first place solution in Tabular Playground January by Danzel [1st place - turn your data into DAEta](https://www.kaggle.com/springmanndaniel/1st-place-turn-your-data-into-daeta/report). It is speculated that this works well since the data itself is artificially created with noise using CTGAN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fundamentals\n",
    "import six\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import scipy\n",
    "import math\n",
    "\n",
    "# data exploration \n",
    "from pandas_profiling import ProfileReport\n",
    "import cufflinks as cf\n",
    "cf.go_offline()\n",
    "cf.set_config_file(offline=True, world_readable=True)\n",
    "from plotly.offline import iplot\n",
    "\n",
    "# data preprocessing \n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, PowerTransformer, MinMaxScaler\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import statsmodels.api as sm\n",
    "\n",
    "#tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv1D\n",
    "from tensorflow.keras.applications import EfficientNetB3\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, UpSampling1D\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "# hyperparameter tuning \n",
    "import kerastuner as kt\n",
    "\n",
    "\n",
    "# metrics for evaluation\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy import stats\n",
    "\n",
    "# saving parameters\n",
    "from joblib import dump, load\n",
    "\n",
    "# hyperparameter searching and tuning \n",
    "import optuna\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fixing random seed for reproducability\n",
    "import random\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df = pd.read_csv('train.csv', index_col = 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv', index_col = 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_col = ['cat0','cat1','cat2','cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8', 'cat9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat0</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>...</th>\n",
       "      <th>cont4</th>\n",
       "      <th>cont5</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>cont11</th>\n",
       "      <th>cont12</th>\n",
       "      <th>cont13</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>C</td>\n",
       "      <td>I</td>\n",
       "      <td>...</td>\n",
       "      <td>0.281421</td>\n",
       "      <td>0.881122</td>\n",
       "      <td>0.421650</td>\n",
       "      <td>0.741413</td>\n",
       "      <td>0.895799</td>\n",
       "      <td>0.802461</td>\n",
       "      <td>0.724417</td>\n",
       "      <td>0.701915</td>\n",
       "      <td>0.877618</td>\n",
       "      <td>0.719903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>A</td>\n",
       "      <td>F</td>\n",
       "      <td>...</td>\n",
       "      <td>0.282354</td>\n",
       "      <td>0.440011</td>\n",
       "      <td>0.346230</td>\n",
       "      <td>0.278495</td>\n",
       "      <td>0.593413</td>\n",
       "      <td>0.546056</td>\n",
       "      <td>0.613252</td>\n",
       "      <td>0.741289</td>\n",
       "      <td>0.326679</td>\n",
       "      <td>0.808464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>N</td>\n",
       "      <td>...</td>\n",
       "      <td>0.293756</td>\n",
       "      <td>0.914155</td>\n",
       "      <td>0.369602</td>\n",
       "      <td>0.832564</td>\n",
       "      <td>0.865620</td>\n",
       "      <td>0.825251</td>\n",
       "      <td>0.264104</td>\n",
       "      <td>0.695561</td>\n",
       "      <td>0.869133</td>\n",
       "      <td>0.828352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>G</td>\n",
       "      <td>K</td>\n",
       "      <td>...</td>\n",
       "      <td>0.769785</td>\n",
       "      <td>0.934138</td>\n",
       "      <td>0.578930</td>\n",
       "      <td>0.407313</td>\n",
       "      <td>0.868099</td>\n",
       "      <td>0.794402</td>\n",
       "      <td>0.494269</td>\n",
       "      <td>0.698125</td>\n",
       "      <td>0.809799</td>\n",
       "      <td>0.614766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>C</td>\n",
       "      <td>F</td>\n",
       "      <td>...</td>\n",
       "      <td>0.279105</td>\n",
       "      <td>0.382600</td>\n",
       "      <td>0.705940</td>\n",
       "      <td>0.325193</td>\n",
       "      <td>0.440967</td>\n",
       "      <td>0.462146</td>\n",
       "      <td>0.724447</td>\n",
       "      <td>0.683073</td>\n",
       "      <td>0.343457</td>\n",
       "      <td>0.297743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499993</th>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>L</td>\n",
       "      <td>...</td>\n",
       "      <td>0.768447</td>\n",
       "      <td>0.269578</td>\n",
       "      <td>0.258655</td>\n",
       "      <td>0.363598</td>\n",
       "      <td>0.300619</td>\n",
       "      <td>0.340516</td>\n",
       "      <td>0.235711</td>\n",
       "      <td>0.383477</td>\n",
       "      <td>0.215227</td>\n",
       "      <td>0.793630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499996</th>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>L</td>\n",
       "      <td>...</td>\n",
       "      <td>0.775951</td>\n",
       "      <td>0.197211</td>\n",
       "      <td>0.257024</td>\n",
       "      <td>0.574304</td>\n",
       "      <td>0.227035</td>\n",
       "      <td>0.322583</td>\n",
       "      <td>0.286094</td>\n",
       "      <td>0.324874</td>\n",
       "      <td>0.306933</td>\n",
       "      <td>0.230902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499997</th>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>C</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>0.297406</td>\n",
       "      <td>0.449482</td>\n",
       "      <td>0.386172</td>\n",
       "      <td>0.476217</td>\n",
       "      <td>0.135947</td>\n",
       "      <td>0.502730</td>\n",
       "      <td>0.235788</td>\n",
       "      <td>0.316671</td>\n",
       "      <td>0.250286</td>\n",
       "      <td>0.349041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499998</th>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>D</td>\n",
       "      <td>E</td>\n",
       "      <td>F</td>\n",
       "      <td>...</td>\n",
       "      <td>0.758642</td>\n",
       "      <td>0.363130</td>\n",
       "      <td>0.324132</td>\n",
       "      <td>0.229017</td>\n",
       "      <td>0.220888</td>\n",
       "      <td>0.515304</td>\n",
       "      <td>0.389391</td>\n",
       "      <td>0.245234</td>\n",
       "      <td>0.303895</td>\n",
       "      <td>0.481138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499999</th>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>C</td>\n",
       "      <td>K</td>\n",
       "      <td>...</td>\n",
       "      <td>0.696047</td>\n",
       "      <td>0.734712</td>\n",
       "      <td>0.404145</td>\n",
       "      <td>0.497719</td>\n",
       "      <td>0.497974</td>\n",
       "      <td>0.782585</td>\n",
       "      <td>0.751251</td>\n",
       "      <td>0.608412</td>\n",
       "      <td>0.712868</td>\n",
       "      <td>0.452400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300000 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       cat0 cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9  ...     cont4  \\\n",
       "id                                                        ...             \n",
       "1         A    B    A    A    B    D    A    E    C    I  ...  0.281421   \n",
       "2         B    A    A    A    B    B    A    E    A    F  ...  0.282354   \n",
       "3         A    A    A    C    B    D    A    B    C    N  ...  0.293756   \n",
       "4         A    A    A    C    B    D    A    E    G    K  ...  0.769785   \n",
       "6         A    B    A    A    B    B    A    E    C    F  ...  0.279105   \n",
       "...     ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...       ...   \n",
       "499993    A    B    A    C    B    B    A    E    E    L  ...  0.768447   \n",
       "499996    A    B    A    C    B    B    A    E    E    L  ...  0.775951   \n",
       "499997    A    B    A    C    B    B    A    E    C    M  ...  0.297406   \n",
       "499998    A    B    B    C    B    B    A    D    E    F  ...  0.758642   \n",
       "499999    A    A    B    A    B    D    A    E    C    K  ...  0.696047   \n",
       "\n",
       "           cont5     cont6     cont7     cont8     cont9    cont10    cont11  \\\n",
       "id                                                                             \n",
       "1       0.881122  0.421650  0.741413  0.895799  0.802461  0.724417  0.701915   \n",
       "2       0.440011  0.346230  0.278495  0.593413  0.546056  0.613252  0.741289   \n",
       "3       0.914155  0.369602  0.832564  0.865620  0.825251  0.264104  0.695561   \n",
       "4       0.934138  0.578930  0.407313  0.868099  0.794402  0.494269  0.698125   \n",
       "6       0.382600  0.705940  0.325193  0.440967  0.462146  0.724447  0.683073   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "499993  0.269578  0.258655  0.363598  0.300619  0.340516  0.235711  0.383477   \n",
       "499996  0.197211  0.257024  0.574304  0.227035  0.322583  0.286094  0.324874   \n",
       "499997  0.449482  0.386172  0.476217  0.135947  0.502730  0.235788  0.316671   \n",
       "499998  0.363130  0.324132  0.229017  0.220888  0.515304  0.389391  0.245234   \n",
       "499999  0.734712  0.404145  0.497719  0.497974  0.782585  0.751251  0.608412   \n",
       "\n",
       "          cont12    cont13  \n",
       "id                          \n",
       "1       0.877618  0.719903  \n",
       "2       0.326679  0.808464  \n",
       "3       0.869133  0.828352  \n",
       "4       0.809799  0.614766  \n",
       "6       0.343457  0.297743  \n",
       "...          ...       ...  \n",
       "499993  0.215227  0.793630  \n",
       "499996  0.306933  0.230902  \n",
       "499997  0.250286  0.349041  \n",
       "499998  0.303895  0.481138  \n",
       "499999  0.712868  0.452400  \n",
       "\n",
       "[300000 rows x 24 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = original_df.drop(columns = 'target', axis =1)\n",
    "Y_train = original_df['target']\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500000, 24)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = pd.concat([X_train, test], axis = 0)\n",
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(handle_unknown='error')\n",
    "# passing max 4 cardinality category column (label encoded values of bridge_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc.fit(X_train[['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5']])\n",
    "enc_df = pd.DataFrame(enc.transform(X_train[['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5']]).toarray())\n",
    "enc_df.index = X_train.index\n",
    "# merge with main df with encoded df on key values\n",
    "X_train = pd.concat([X_train, enc_df], ignore_index = False, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_df = pd.DataFrame(enc.transform(test[['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5']]).toarray())\n",
    "enc_df.index = test.index\n",
    "# merge with main df with encoded df on key values\n",
    "test = pd.concat([test, enc_df], ignore_index = False, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_df = pd.DataFrame(enc.transform(input_data[['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5']]).toarray())\n",
    "enc_df.index = input_data.index\n",
    "# merge with main df with encoded df on key values\n",
    "input_data = pd.concat([input_data, enc_df], ignore_index = False, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat0</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>...</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>C</td>\n",
       "      <td>I</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>A</td>\n",
       "      <td>F</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>N</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>G</td>\n",
       "      <td>K</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>C</td>\n",
       "      <td>F</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499987</th>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>G</td>\n",
       "      <td>L</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499990</th>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>F</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499991</th>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>C</td>\n",
       "      <td>K</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499994</th>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>C</td>\n",
       "      <td>F</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499995</th>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>G</td>\n",
       "      <td>H</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500000 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       cat0 cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9  ...    8    9   10  \\\n",
       "id                                                        ...                  \n",
       "1         A    B    A    A    B    D    A    E    C    I  ...  0.0  0.0  0.0   \n",
       "2         B    A    A    A    B    B    A    E    A    F  ...  0.0  0.0  0.0   \n",
       "3         A    A    A    C    B    D    A    B    C    N  ...  1.0  0.0  0.0   \n",
       "4         A    A    A    C    B    D    A    E    G    K  ...  1.0  0.0  0.0   \n",
       "6         A    B    A    A    B    B    A    E    C    F  ...  0.0  0.0  0.0   \n",
       "...     ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "499987    A    A    A    C    B    D    A    E    G    L  ...  1.0  0.0  0.0   \n",
       "499990    A    A    A    C    B    D    A    E    E    F  ...  1.0  0.0  0.0   \n",
       "499991    A    A    A    C    B    D    A    E    C    K  ...  1.0  0.0  0.0   \n",
       "499994    A    B    A    A    B    D    A    E    C    F  ...  0.0  0.0  0.0   \n",
       "499995    A    B    A    C    B    C    A    E    G    H  ...  1.0  0.0  0.0   \n",
       "\n",
       "         11   12   13   14   15   16   17  \n",
       "id                                         \n",
       "1       1.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
       "2       1.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
       "3       1.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
       "4       1.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
       "6       1.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
       "...     ...  ...  ...  ...  ...  ...  ...  \n",
       "499987  1.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
       "499990  1.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
       "499991  1.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
       "499994  1.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
       "499995  1.0  0.0  0.0  0.0  0.0  1.0  0.0  \n",
       "\n",
       "[500000 rows x 42 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>cont0</th>\n",
       "      <th>cont1</th>\n",
       "      <th>cont2</th>\n",
       "      <th>cont3</th>\n",
       "      <th>cont4</th>\n",
       "      <th>cont5</th>\n",
       "      <th>...</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>C</td>\n",
       "      <td>I</td>\n",
       "      <td>0.923191</td>\n",
       "      <td>0.684968</td>\n",
       "      <td>0.124454</td>\n",
       "      <td>0.217886</td>\n",
       "      <td>0.281421</td>\n",
       "      <td>0.881122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>A</td>\n",
       "      <td>F</td>\n",
       "      <td>0.437627</td>\n",
       "      <td>0.014213</td>\n",
       "      <td>0.357438</td>\n",
       "      <td>0.846127</td>\n",
       "      <td>0.282354</td>\n",
       "      <td>0.440011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>N</td>\n",
       "      <td>0.732209</td>\n",
       "      <td>0.760122</td>\n",
       "      <td>0.454644</td>\n",
       "      <td>0.812990</td>\n",
       "      <td>0.293756</td>\n",
       "      <td>0.914155</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>G</td>\n",
       "      <td>K</td>\n",
       "      <td>0.705142</td>\n",
       "      <td>0.771678</td>\n",
       "      <td>0.153735</td>\n",
       "      <td>0.732893</td>\n",
       "      <td>0.769785</td>\n",
       "      <td>0.934138</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>C</td>\n",
       "      <td>F</td>\n",
       "      <td>0.486063</td>\n",
       "      <td>0.639349</td>\n",
       "      <td>0.496212</td>\n",
       "      <td>0.354186</td>\n",
       "      <td>0.279105</td>\n",
       "      <td>0.382600</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499987</th>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>G</td>\n",
       "      <td>L</td>\n",
       "      <td>0.919265</td>\n",
       "      <td>0.111147</td>\n",
       "      <td>0.199583</td>\n",
       "      <td>0.181354</td>\n",
       "      <td>0.277365</td>\n",
       "      <td>0.963678</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499990</th>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>F</td>\n",
       "      <td>0.362875</td>\n",
       "      <td>-0.020197</td>\n",
       "      <td>0.469025</td>\n",
       "      <td>0.336185</td>\n",
       "      <td>0.523174</td>\n",
       "      <td>0.232072</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499991</th>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>C</td>\n",
       "      <td>K</td>\n",
       "      <td>0.463060</td>\n",
       "      <td>0.740421</td>\n",
       "      <td>0.446293</td>\n",
       "      <td>0.411387</td>\n",
       "      <td>0.517103</td>\n",
       "      <td>0.432927</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499994</th>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>C</td>\n",
       "      <td>F</td>\n",
       "      <td>0.708709</td>\n",
       "      <td>0.418490</td>\n",
       "      <td>0.193004</td>\n",
       "      <td>0.862700</td>\n",
       "      <td>0.279153</td>\n",
       "      <td>0.837712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499995</th>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>G</td>\n",
       "      <td>H</td>\n",
       "      <td>0.646432</td>\n",
       "      <td>0.484954</td>\n",
       "      <td>0.141289</td>\n",
       "      <td>0.753487</td>\n",
       "      <td>0.763246</td>\n",
       "      <td>0.792263</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500000 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       cat6 cat7 cat8 cat9     cont0     cont1     cont2     cont3     cont4  \\\n",
       "id                                                                             \n",
       "1         A    E    C    I  0.923191  0.684968  0.124454  0.217886  0.281421   \n",
       "2         A    E    A    F  0.437627  0.014213  0.357438  0.846127  0.282354   \n",
       "3         A    B    C    N  0.732209  0.760122  0.454644  0.812990  0.293756   \n",
       "4         A    E    G    K  0.705142  0.771678  0.153735  0.732893  0.769785   \n",
       "6         A    E    C    F  0.486063  0.639349  0.496212  0.354186  0.279105   \n",
       "...     ...  ...  ...  ...       ...       ...       ...       ...       ...   \n",
       "499987    A    E    G    L  0.919265  0.111147  0.199583  0.181354  0.277365   \n",
       "499990    A    E    E    F  0.362875 -0.020197  0.469025  0.336185  0.523174   \n",
       "499991    A    E    C    K  0.463060  0.740421  0.446293  0.411387  0.517103   \n",
       "499994    A    E    C    F  0.708709  0.418490  0.193004  0.862700  0.279153   \n",
       "499995    A    E    G    H  0.646432  0.484954  0.141289  0.753487  0.763246   \n",
       "\n",
       "           cont5  ...    8    9   10   11   12   13   14   15   16   17  \n",
       "id                ...                                                    \n",
       "1       0.881122  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
       "2       0.440011  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
       "3       0.914155  ...  1.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
       "4       0.934138  ...  1.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
       "6       0.382600  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
       "...          ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "499987  0.963678  ...  1.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
       "499990  0.232072  ...  1.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
       "499991  0.432927  ...  1.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
       "499994  0.837712  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
       "499995  0.792263  ...  1.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  1.0  0.0  \n",
       "\n",
       "[500000 rows x 36 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for df in [X_train, test, input_data]:\n",
    "    df.drop(columns = ['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5'], inplace = True)\n",
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = OrdinalEncoder()\n",
    "le.fit(X_train[['cat6', 'cat7', 'cat8','cat9']])\n",
    "X_train[['cat6', 'cat7', 'cat8','cat9']] = le.transform(X_train[['cat6', 'cat7', 'cat8','cat9']])\n",
    "test[['cat6', 'cat7', 'cat8','cat9']] = le.transform(test[['cat6', 'cat7', 'cat8','cat9']])\n",
    "input_data[['cat6', 'cat7', 'cat8','cat9']] = le.transform(input_data[['cat6', 'cat7', 'cat8','cat9']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>cont0</th>\n",
       "      <th>cont1</th>\n",
       "      <th>cont2</th>\n",
       "      <th>cont3</th>\n",
       "      <th>cont4</th>\n",
       "      <th>cont5</th>\n",
       "      <th>...</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.923191</td>\n",
       "      <td>0.684968</td>\n",
       "      <td>0.124454</td>\n",
       "      <td>0.217886</td>\n",
       "      <td>0.281421</td>\n",
       "      <td>0.881122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.437627</td>\n",
       "      <td>0.014213</td>\n",
       "      <td>0.357438</td>\n",
       "      <td>0.846127</td>\n",
       "      <td>0.282354</td>\n",
       "      <td>0.440011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.732209</td>\n",
       "      <td>0.760122</td>\n",
       "      <td>0.454644</td>\n",
       "      <td>0.812990</td>\n",
       "      <td>0.293756</td>\n",
       "      <td>0.914155</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.705142</td>\n",
       "      <td>0.771678</td>\n",
       "      <td>0.153735</td>\n",
       "      <td>0.732893</td>\n",
       "      <td>0.769785</td>\n",
       "      <td>0.934138</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.486063</td>\n",
       "      <td>0.639349</td>\n",
       "      <td>0.496212</td>\n",
       "      <td>0.354186</td>\n",
       "      <td>0.279105</td>\n",
       "      <td>0.382600</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499987</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.919265</td>\n",
       "      <td>0.111147</td>\n",
       "      <td>0.199583</td>\n",
       "      <td>0.181354</td>\n",
       "      <td>0.277365</td>\n",
       "      <td>0.963678</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499990</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.362875</td>\n",
       "      <td>-0.020197</td>\n",
       "      <td>0.469025</td>\n",
       "      <td>0.336185</td>\n",
       "      <td>0.523174</td>\n",
       "      <td>0.232072</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499991</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.463060</td>\n",
       "      <td>0.740421</td>\n",
       "      <td>0.446293</td>\n",
       "      <td>0.411387</td>\n",
       "      <td>0.517103</td>\n",
       "      <td>0.432927</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499994</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.708709</td>\n",
       "      <td>0.418490</td>\n",
       "      <td>0.193004</td>\n",
       "      <td>0.862700</td>\n",
       "      <td>0.279153</td>\n",
       "      <td>0.837712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.646432</td>\n",
       "      <td>0.484954</td>\n",
       "      <td>0.141289</td>\n",
       "      <td>0.753487</td>\n",
       "      <td>0.763246</td>\n",
       "      <td>0.792263</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500000 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        cat6  cat7  cat8  cat9     cont0     cont1     cont2     cont3  \\\n",
       "id                                                                       \n",
       "1        0.0   4.0   2.0   8.0  0.923191  0.684968  0.124454  0.217886   \n",
       "2        0.0   4.0   0.0   5.0  0.437627  0.014213  0.357438  0.846127   \n",
       "3        0.0   1.0   2.0  13.0  0.732209  0.760122  0.454644  0.812990   \n",
       "4        0.0   4.0   6.0  10.0  0.705142  0.771678  0.153735  0.732893   \n",
       "6        0.0   4.0   2.0   5.0  0.486063  0.639349  0.496212  0.354186   \n",
       "...      ...   ...   ...   ...       ...       ...       ...       ...   \n",
       "499987   0.0   4.0   6.0  11.0  0.919265  0.111147  0.199583  0.181354   \n",
       "499990   0.0   4.0   4.0   5.0  0.362875 -0.020197  0.469025  0.336185   \n",
       "499991   0.0   4.0   2.0  10.0  0.463060  0.740421  0.446293  0.411387   \n",
       "499994   0.0   4.0   2.0   5.0  0.708709  0.418490  0.193004  0.862700   \n",
       "499995   0.0   4.0   6.0   7.0  0.646432  0.484954  0.141289  0.753487   \n",
       "\n",
       "           cont4     cont5  ...    8    9   10   11   12   13   14   15   16  \\\n",
       "id                          ...                                                \n",
       "1       0.281421  0.881122  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0   \n",
       "2       0.282354  0.440011  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  1.0  0.0   \n",
       "3       0.293756  0.914155  ...  1.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0   \n",
       "4       0.769785  0.934138  ...  1.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0   \n",
       "6       0.279105  0.382600  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  1.0  0.0   \n",
       "...          ...       ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "499987  0.277365  0.963678  ...  1.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0   \n",
       "499990  0.523174  0.232072  ...  1.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0   \n",
       "499991  0.517103  0.432927  ...  1.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0   \n",
       "499994  0.279153  0.837712  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0   \n",
       "499995  0.763246  0.792263  ...  1.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  1.0   \n",
       "\n",
       "         17  \n",
       "id           \n",
       "1       1.0  \n",
       "2       0.0  \n",
       "3       1.0  \n",
       "4       1.0  \n",
       "6       0.0  \n",
       "...     ...  \n",
       "499987  1.0  \n",
       "499990  1.0  \n",
       "499991  1.0  \n",
       "499994  1.0  \n",
       "499995  0.0  \n",
       "\n",
       "[500000 rows x 36 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_columns = len(input_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minmax scaling\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "test = scaler.transform(test)\n",
    "input_data = scaler.transform(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inputSwapNoiseTEST(arr, p, randomise_type = 'random'):\n",
    "    '''\n",
    "    Takes a numpy array and swaps a row of each \n",
    "    feature with another value from the same column with probability p\n",
    "    '''\n",
    "\n",
    "    n, m = arr.shape #row, col\n",
    "    idx = range(n)\n",
    "    swap_n = round(n*p)\n",
    "    arr2 = arr.copy()\n",
    "\n",
    "    if randomise_type == 'random':\n",
    "        for i in range(m):\n",
    "            col_vals = np.random.permutation(arr[:, i]) \n",
    "            swap_idx = np.random.choice(idx, size= swap_n) \n",
    "            arr2[swap_idx, i] = np.random.choice(col_vals, size = swap_n) # n*p row and change it \n",
    "            \n",
    "    elif randomise_type == 'row':\n",
    "        row_ref = np.random.permutation(n) # change the order of the row\n",
    "        for i in range(n):\n",
    "            \n",
    "            arr2[i,:] = np.random.choice(col_vals, size = swap_n) # n*p row and change it \n",
    "            \n",
    "    return arr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3,  4,  5],\n",
       "       [ 6,  7,  8,  9, 10],\n",
       "       [11, 12, 13, 14, 15],\n",
       "       [16, 17, 18, 19, 20],\n",
       "       [21, 22, 23, 24, 25],\n",
       "       [26, 27, 28, 29, 30]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = np.arange(1,31).reshape(6,5)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3, 14,  5],\n",
       "       [ 6,  7,  8,  9, 10],\n",
       "       [11, 12, 13, 14, 15],\n",
       "       [16, 17, 18, 19, 25],\n",
       "       [21, 22, 23, 24, 25],\n",
       "       [ 6, 27, 28, 29, 30]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix2 =  inputSwapNoiseTEST(matrix, p = 0.2, randomise_type = 'random')\n",
    "matrix2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500000, 36)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KFoldAverageDAE:\n",
    "    def __init__(self, FOLDS):\n",
    "        # creating CV folds for training \n",
    "        self.models = []\n",
    "        self.kfolds = KFold(n_splits = FOLDS, \n",
    "                            shuffle = True,\n",
    "                            random_state = 50)\n",
    "        \n",
    "        \n",
    "    def get_autoencoder_model(self, train_x):\n",
    "        \"\"\"\n",
    "        transform the 1 row of 36 features (1 * 36) into (3 * 800) layers autoencoder\n",
    "        \n",
    "        \"\"\"\n",
    "        len_input_output = train_x.shape[-1]\n",
    "        self.input_ = keras.Input((len_input_output,), name = 'original_data')\n",
    "        self.encode1 = Dense(units = 1024, activation='swish')(self.input_)\n",
    "        self.encode2 = Dense(units = 512, activation='swish')(self.encode1)\n",
    "        self.bottleneck = Dense(units=64, activation='linear')(self.encode2)\n",
    "        \n",
    "        # Use only the encoder part for denoising \n",
    "        self.encoder = keras.Model(inputs= self.input_, outputs= (self.bottleneck), name = \"encoder\")   \n",
    "        self.encoder.summary()\n",
    "        \n",
    "        \n",
    "        self.decoder_input = keras.Input(shape = (64,), name = 'encoded_data')\n",
    "        self.decoder1 = Dense(units = 512, activation='swish')(self.decoder_input)\n",
    "        self.decoder2 = Dense(units = 1024, activation='swish')(self.decoder1)\n",
    "        self.decoder_output = Dense(units = len_input_output, activation = 'linear')(self.decoder2)\n",
    "        self.decoder = keras.Model(self.decoder_input, self.decoder_output, name = \"decoders\")\n",
    "        self.decoder.summary()\n",
    "        \n",
    "        # Training is performed on the entire autoencoder\n",
    "        \n",
    "        self.autoencoder_input = keras.Input((len_input_output,), name = 'data')\n",
    "        self.encoded_data = self.encoder(self.autoencoder_input)\n",
    "        self.decoded_data = self.decoder(self.encoded_data)\n",
    "        self.autoencoder = keras.Model(inputs = self.autoencoder_input, \n",
    "                                       outputs= self.decoded_data,\n",
    "                                      name = \"autoencoder\")\n",
    "        \n",
    "        \n",
    "        self.autoencoder.summary()\n",
    "        \n",
    "        self.autoencoder.compile(optimizer = Adam(learning_rate = 0.1), loss = 'mse', metrics = 'mse')\n",
    "        \n",
    "        model = self\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def inputSwapNoise(self, arr, p, randomise_type = 'random'):\n",
    "        '''\n",
    "        Takes a numpy array and swaps a row of each \n",
    "        feature with another value from the same column with probability p\n",
    "        '''\n",
    "\n",
    "        n, m = arr.shape\n",
    "        idx = range(n)\n",
    "        swap_n = round(n*p)\n",
    "        arr2 = arr.copy()\n",
    "        for i in range(m):\n",
    "            col_vals = np.random.permutation(arr[:, i]) # change the order of the row\n",
    "            swap_idx = np.random.choice(idx, size= swap_n) # choose row\n",
    "            arr2[swap_idx, i] = np.random.choice(col_vals, size = swap_n) # n*p row and change it \n",
    "        return arr2\n",
    "        \n",
    "    def autoencoder_fit(self, INPUT_DATA, BATCH_SIZE, EPOCHS, p, randomise_type = 'random'):\n",
    "        \n",
    "        noised_data = self.inputSwapNoise(INPUT_DATA, p)\n",
    "\n",
    "        oof_preds = np.zeros_like(INPUT_DATA)\n",
    "        self.noised_data = pd.DataFrame(noised_data).values\n",
    "        self.INPUT_DATA = pd.DataFrame(INPUT_DATA).values\n",
    "        \n",
    "        \n",
    "        # adding callbacks\n",
    "        model_save = ModelCheckpoint('./best_DAE_model.h5', \n",
    "                             save_best_only = True, \n",
    "                             save_weights_only = True,\n",
    "                             monitor = 'val_loss', \n",
    "                             mode = 'min', verbose = 10)\n",
    "        early_stop = EarlyStopping(monitor = 'val_loss', min_delta = 0.0005, \n",
    "                           patience = 10, mode = 'min', verbose = 10,\n",
    "                           restore_best_weights = True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.1, \n",
    "                              patience = 5, min_delta = 0.0005, \n",
    "                              mode = 'min', verbose = 10)\n",
    "        \n",
    "        \n",
    "        for train_idx, val_idx in self.kfolds.split(INPUT_DATA):\n",
    "            noised_data_train_CV, noised_data_val_CV = self.noised_data[train_idx], self.noised_data[val_idx]\n",
    "            INPUT_DATA_train_CV, INPUT_DATA_val_CV = self.INPUT_DATA[train_idx], self.INPUT_DATA[val_idx]\n",
    "            \n",
    "                \n",
    "            model.autoencoder.fit(x = noised_data_train_CV, \n",
    "                                      y = INPUT_DATA_train_CV, \n",
    "                                      batch_size = BATCH_SIZE,\n",
    "                                      validation_data = (noised_data_val_CV,INPUT_DATA_val_CV),\n",
    "                                      epochs = EPOCHS, \n",
    "                                      callbacks = [model_save, early_stop, reduce_lr])\n",
    "            \n",
    "            \n",
    "            self.models.append(model)\n",
    "            oof_pred = self.autoencoder.predict(noised_data_val_CV)\n",
    "            oof_preds[val_idx] = oof_pred\n",
    "\n",
    "        self.oof_preds = oof_preds\n",
    "        \n",
    "        self.rmse = mean_squared_error(INPUT_DATA, oof_preds, squared = False)\n",
    "        \n",
    "    def encoder_predict(self, test_x):\n",
    "        preds = []\n",
    "        for model in tqdm.tqdm(self.models):\n",
    "            pred = model.encoder.predict(test_x)\n",
    "            preds.append(pred)\n",
    "        preds = np.mean(preds, axis=0)\n",
    "        return preds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.57142857, 0.33333333, ..., 0.        , 0.        ,\n",
       "        1.        ],\n",
       "       [0.        , 0.57142857, 0.        , ..., 1.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.14285714, 0.33333333, ..., 0.        , 0.        ,\n",
       "        1.        ],\n",
       "       ...,\n",
       "       [0.        , 0.57142857, 0.33333333, ..., 1.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.42857143, 0.66666667, ..., 1.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.57142857, 0.33333333, ..., 0.        , 0.        ,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KFoldAverageDAE(FOLDS = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "original_data (InputLayer)   [(None, 36)]              0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 1024)              37888     \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 64)                32832     \n",
      "=================================================================\n",
      "Total params: 595,520\n",
      "Trainable params: 595,520\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"decoders\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoded_data (InputLayer)    [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 512)               33280     \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 36)                36900     \n",
      "=================================================================\n",
      "Total params: 595,492\n",
      "Trainable params: 595,492\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"autoencoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "data (InputLayer)            [(None, 36)]              0         \n",
      "_________________________________________________________________\n",
      "encoder (Functional)         (None, 64)                595520    \n",
      "_________________________________________________________________\n",
      "decoders (Functional)        (None, 36)                595492    \n",
      "=================================================================\n",
      "Total params: 1,191,012\n",
      "Trainable params: 1,191,012\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = model.get_autoencoder_model(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.utils.plot_model(model.encoder, \"encoder.png\", show_shapes=True, show_layer_names = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.utils.plot_model(model.decoder, \"decoder.png\", show_shapes=True, show_layer_names = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.utils.plot_model(model.autoencoder, \"DAE.png\", show_shapes=True, show_layer_names = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "6250/6250 [==============================] - 29s 5ms/step - loss: 329349341248.9799 - mse: 329349341248.9799 - val_loss: 0.1473 - val_mse: 0.1473\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.14727, saving model to .\\best_DAE_model.h5\n",
      "Epoch 2/1000\n",
      "6250/6250 [==============================] - 28s 5ms/step - loss: 0.1450 - mse: 0.1450 - val_loss: 0.1242 - val_mse: 0.1242\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.14727 to 0.12425, saving model to .\\best_DAE_model.h5\n",
      "Epoch 3/1000\n",
      "6250/6250 [==============================] - 29s 5ms/step - loss: 0.1066 - mse: 0.1066 - val_loss: 0.0755 - val_mse: 0.0755\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.12425 to 0.07549, saving model to .\\best_DAE_model.h5\n",
      "Epoch 4/1000\n",
      "6250/6250 [==============================] - 28s 4ms/step - loss: 0.0756 - mse: 0.0756 - val_loss: 0.0755 - val_mse: 0.0755\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.07549\n",
      "Epoch 5/1000\n",
      "6250/6250 [==============================] - 30s 5ms/step - loss: 0.0757 - mse: 0.0757 - val_loss: 0.0759 - val_mse: 0.0759\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.07549\n",
      "Epoch 6/1000\n",
      "6250/6250 [==============================] - 31s 5ms/step - loss: 0.0766 - mse: 0.0766 - val_loss: 0.0765 - val_mse: 0.0765\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.07549\n",
      "Epoch 7/1000\n",
      "6250/6250 [==============================] - 30s 5ms/step - loss: 0.0767 - mse: 0.0767 - val_loss: 0.0758 - val_mse: 0.0758\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.07549\n",
      "Epoch 8/1000\n",
      "6250/6250 [==============================] - 29s 5ms/step - loss: 0.0768 - mse: 0.0768 - val_loss: 0.0762 - val_mse: 0.0762\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.07549\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.010000000149011612.\n",
      "Epoch 9/1000\n",
      "6250/6250 [==============================] - 29s 5ms/step - loss: 0.0758 - mse: 0.0758 - val_loss: 0.0758 - val_mse: 0.0758\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.07549\n",
      "Epoch 10/1000\n",
      "6250/6250 [==============================] - 28s 5ms/step - loss: 0.0758 - mse: 0.0758 - val_loss: 0.0756 - val_mse: 0.0756\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.07549\n",
      "Epoch 11/1000\n",
      "6250/6250 [==============================] - 29s 5ms/step - loss: 0.0757 - mse: 0.0757 - val_loss: 0.0757 - val_mse: 0.0757\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.07549\n",
      "Epoch 12/1000\n",
      "6250/6250 [==============================] - 29s 5ms/step - loss: 0.0758 - mse: 0.0758 - val_loss: 0.0756 - val_mse: 0.0756\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.07549\n",
      "Epoch 13/1000\n",
      "6250/6250 [==============================] - 29s 5ms/step - loss: 0.0757 - mse: 0.0757 - val_loss: 0.0755 - val_mse: 0.0755\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.07549\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 00013: early stopping\n",
      "Epoch 1/1000\n",
      "6250/6250 [==============================] - 29s 5ms/step - loss: 0.0756 - mse: 0.0756 - val_loss: 0.0756 - val_mse: 0.0756\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.07549\n",
      "Epoch 2/1000\n",
      "6250/6250 [==============================] - 29s 5ms/step - loss: 0.0756 - mse: 0.0756 - val_loss: 0.0756 - val_mse: 0.0756\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.07549\n",
      "Epoch 3/1000\n",
      "6250/6250 [==============================] - 29s 5ms/step - loss: 0.0756 - mse: 0.0756 - val_loss: 0.0756 - val_mse: 0.0756\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.07549\n",
      "Epoch 4/1000\n",
      "6250/6250 [==============================] - 29s 5ms/step - loss: 0.0756 - mse: 0.0756 - val_loss: 0.0756 - val_mse: 0.0756\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.07549\n",
      "Epoch 5/1000\n",
      "6250/6250 [==============================] - 29s 5ms/step - loss: 0.0756 - mse: 0.0756 - val_loss: 0.0756 - val_mse: 0.0756\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.07549\n",
      "Epoch 6/1000\n",
      "6250/6250 [==============================] - 29s 5ms/step - loss: 0.0756 - mse: 0.0756 - val_loss: 0.0756 - val_mse: 0.0756\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.07549\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 7/1000\n",
      "6250/6250 [==============================] - 28s 4ms/step - loss: 0.0756 - mse: 0.0756 - val_loss: 0.0756 - val_mse: 0.0756\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.07549\n",
      "Epoch 8/1000\n",
      "6250/6250 [==============================] - 28s 4ms/step - loss: 0.0756 - mse: 0.0756 - val_loss: 0.0756 - val_mse: 0.0756\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.07549\n",
      "Epoch 9/1000\n",
      "6250/6250 [==============================] - 28s 4ms/step - loss: 0.0756 - mse: 0.0756 - val_loss: 0.0756 - val_mse: 0.0756\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.07549\n",
      "Epoch 10/1000\n",
      "6250/6250 [==============================] - 30s 5ms/step - loss: 0.0756 - mse: 0.0756 - val_loss: 0.0756 - val_mse: 0.0756\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.07549\n",
      "Epoch 11/1000\n",
      "6250/6250 [==============================] - 30s 5ms/step - loss: 0.0756 - mse: 0.0756 - val_loss: 0.0756 - val_mse: 0.0756\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.07549\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 00011: early stopping\n",
      "Epoch 1/1000\n",
      "6250/6250 [==============================] - 30s 5ms/step - loss: 0.0756 - mse: 0.0756 - val_loss: 0.0756 - val_mse: 0.0756\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.07549\n",
      "Epoch 2/1000\n",
      "6250/6250 [==============================] - 30s 5ms/step - loss: 0.0756 - mse: 0.0756 - val_loss: 0.0756 - val_mse: 0.0756\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.07549\n",
      "Epoch 3/1000\n",
      "6250/6250 [==============================] - 28s 5ms/step - loss: 0.0756 - mse: 0.0756 - val_loss: 0.0756 - val_mse: 0.0756\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.07549\n",
      "Epoch 4/1000\n",
      "6250/6250 [==============================] - 27s 4ms/step - loss: 0.0756 - mse: 0.0756 - val_loss: 0.0756 - val_mse: 0.0756\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.07549\n",
      "Epoch 5/1000\n",
      "6250/6250 [==============================] - 26s 4ms/step - loss: 0.0756 - mse: 0.0756 - val_loss: 0.0756 - val_mse: 0.0756\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.07549\n",
      "Epoch 6/1000\n",
      "6250/6250 [==============================] - 25s 4ms/step - loss: 0.0756 - mse: 0.0756 - val_loss: 0.0756 - val_mse: 0.0756\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.07549\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 7/1000\n",
      "6250/6250 [==============================] - 28s 4ms/step - loss: 0.0756 - mse: 0.0756 - val_loss: 0.0756 - val_mse: 0.0756\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.07549\n",
      "Epoch 8/1000\n",
      "6250/6250 [==============================] - 26s 4ms/step - loss: 0.0756 - mse: 0.0756 - val_loss: 0.0756 - val_mse: 0.0756\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.07549\n",
      "Epoch 9/1000\n",
      "6250/6250 [==============================] - 26s 4ms/step - loss: 0.0756 - mse: 0.0756 - val_loss: 0.0756 - val_mse: 0.0756\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.07549\n",
      "Epoch 10/1000\n",
      "6250/6250 [==============================] - 26s 4ms/step - loss: 0.0756 - mse: 0.0756 - val_loss: 0.0756 - val_mse: 0.0756\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.07549\n",
      "Epoch 11/1000\n",
      "6250/6250 [==============================] - 26s 4ms/step - loss: 0.0756 - mse: 0.0756 - val_loss: 0.0756 - val_mse: 0.0756\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.07549\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "Epoch 00011: early stopping\n",
      "Epoch 1/1000\n",
      "6250/6250 [==============================] - 25s 4ms/step - loss: 0.0756 - mse: 0.0756 - val_loss: 0.0757 - val_mse: 0.0757\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.07549\n",
      "Epoch 2/1000\n",
      "6250/6250 [==============================] - 25s 4ms/step - loss: 0.0756 - mse: 0.0756 - val_loss: 0.0757 - val_mse: 0.0757\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.07549\n",
      "Epoch 3/1000\n",
      "6250/6250 [==============================] - 25s 4ms/step - loss: 0.0756 - mse: 0.0756 - val_loss: 0.0757 - val_mse: 0.0757\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.07549\n",
      "Epoch 4/1000\n",
      "6250/6250 [==============================] - 25s 4ms/step - loss: 0.0756 - mse: 0.0756 - val_loss: 0.0757 - val_mse: 0.0757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00004: val_loss did not improve from 0.07549\n",
      "Epoch 5/1000\n",
      "6250/6250 [==============================] - 25s 4ms/step - loss: 0.0756 - mse: 0.0756 - val_loss: 0.0757 - val_mse: 0.0757\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.07549\n",
      "Epoch 6/1000\n",
      "6250/6250 [==============================] - 25s 4ms/step - loss: 0.0756 - mse: 0.0756 - val_loss: 0.0757 - val_mse: 0.0757\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.07549\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 9.999998695775504e-09.\n",
      "Epoch 7/1000\n",
      "6250/6250 [==============================] - 25s 4ms/step - loss: 0.0756 - mse: 0.0756 - val_loss: 0.0757 - val_mse: 0.0757\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.07549\n",
      "Epoch 8/1000\n",
      "6250/6250 [==============================] - 25s 4ms/step - loss: 0.0756 - mse: 0.0756 - val_loss: 0.0757 - val_mse: 0.0757\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.07549\n",
      "Epoch 9/1000\n",
      "6250/6250 [==============================] - 25s 4ms/step - loss: 0.0756 - mse: 0.0756 - val_loss: 0.0757 - val_mse: 0.0757\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.07549\n",
      "Epoch 10/1000\n",
      "6250/6250 [==============================] - 25s 4ms/step - loss: 0.0756 - mse: 0.0756 - val_loss: 0.0757 - val_mse: 0.0757\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.07549\n",
      "Epoch 11/1000\n",
      "6250/6250 [==============================] - 25s 4ms/step - loss: 0.0756 - mse: 0.0756 - val_loss: 0.0757 - val_mse: 0.0757\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.07549\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.99999905104687e-10.\n",
      "Epoch 00011: early stopping\n",
      "Epoch 1/1000\n",
      "6250/6250 [==============================] - 26s 4ms/step - loss: 0.0756 - mse: 0.0756 - val_loss: 0.0758 - val_mse: 0.0758\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.07549\n",
      "Epoch 2/1000\n",
      "6250/6250 [==============================] - 25s 4ms/step - loss: 0.0756 - mse: 0.0756 - val_loss: 0.0758 - val_mse: 0.0758\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.07549\n",
      "Epoch 3/1000\n",
      "6250/6250 [==============================] - 25s 4ms/step - loss: 0.0756 - mse: 0.0756 - val_loss: 0.0758 - val_mse: 0.0758\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.07549\n",
      "Epoch 4/1000\n",
      "6250/6250 [==============================] - 25s 4ms/step - loss: 0.0756 - mse: 0.0756 - val_loss: 0.0758 - val_mse: 0.0758\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.07549\n",
      "Epoch 5/1000\n",
      "6250/6250 [==============================] - 25s 4ms/step - loss: 0.0756 - mse: 0.0756 - val_loss: 0.0758 - val_mse: 0.0758\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.07549\n",
      "Epoch 6/1000\n",
      "6250/6250 [==============================] - 25s 4ms/step - loss: 0.0756 - mse: 0.0756 - val_loss: 0.0758 - val_mse: 0.0758\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.07549\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 9.999998606957661e-11.\n",
      "Epoch 7/1000\n",
      "6250/6250 [==============================] - 25s 4ms/step - loss: 0.0756 - mse: 0.0756 - val_loss: 0.0758 - val_mse: 0.0758\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.07549\n",
      "Epoch 8/1000\n",
      "6250/6250 [==============================] - 25s 4ms/step - loss: 0.0756 - mse: 0.0756 - val_loss: 0.0758 - val_mse: 0.0758\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.07549\n",
      "Epoch 9/1000\n",
      "6250/6250 [==============================] - 25s 4ms/step - loss: 0.0756 - mse: 0.0756 - val_loss: 0.0758 - val_mse: 0.0758\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.07549\n",
      "Epoch 10/1000\n",
      "6250/6250 [==============================] - 25s 4ms/step - loss: 0.0756 - mse: 0.0756 - val_loss: 0.0758 - val_mse: 0.0758\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.07549\n",
      "Epoch 11/1000\n",
      "6250/6250 [==============================] - 25s 4ms/step - loss: 0.0756 - mse: 0.0756 - val_loss: 0.0758 - val_mse: 0.0758\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.07549\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.99999874573554e-12.\n",
      "Epoch 00011: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.autoencoder_fit(input_data, \n",
    "                      BATCH_SIZE = 64, \n",
    "                      EPOCHS = 1000,\n",
    "                      p = 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model.rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:29<00:00,  5.93s/it]\n"
     ]
    }
   ],
   "source": [
    "encoded_X_train = model.encoder_predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.57142857, 0.33333333, 0.57142857, 0.88703657,\n",
       "       0.8160911 , 0.17136704, 0.09536034, 0.11456403, 0.85569127,\n",
       "       0.35993147, 0.64349616, 0.88282901, 0.81132239, 0.68524143,\n",
       "       0.65485395, 0.91338699, 0.78520634, 1.        , 0.        ,\n",
       "       0.        , 1.        , 1.        , 0.        , 1.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       1.        ])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  391.07187 ,  1434.0673  ,  -764.60156 ,   155.41956 ,\n",
       "         364.7265  ,   902.9403  ,   640.0755  ,  -108.6369  ,\n",
       "       -1329.6902  , -1305.2806  , -1278.6647  ,    98.203896,\n",
       "        -193.96558 ,   294.35324 , -1601.0829  ,  2601.542   ,\n",
       "        1705.7312  ,  -167.5933  , -1770.8093  ,   792.3981  ,\n",
       "        -234.64929 , -1090.4094  ,  1466.0442  ,  2672.2476  ,\n",
       "         696.6223  ,  1584.5569  ,   924.41144 ,  2004.0944  ,\n",
       "        -876.7328  ,  -232.90364 ,   466.19257 ,  1232.441   ,\n",
       "        -135.53796 , -2034.4417  ,  2746.304   , -1130.4816  ,\n",
       "         311.1975  , -1109.4451  ,   232.1073  ,  -584.5104  ,\n",
       "        1752.1752  ,  1333.5214  , -1342.1704  ,   216.02583 ,\n",
       "       -2024.3635  ,  1692.3096  ,   -16.456614, -1321.4747  ,\n",
       "         476.2022  , -1121.1282  ,  3102.898   ,   186.41667 ,\n",
       "        1581.026   ,  -224.80283 , -2918.3257  ,   -77.200645,\n",
       "         625.3376  , -2618.1616  ,  2240.9702  ,  1576.1901  ,\n",
       "         753.0226  , -1727.0164  , -1729.0023  , -1427.2401  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_X_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['encoded_X_train.joblib']"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(encoded_X_train, \"encoded_X_train.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:19<00:00,  3.80s/it]\n"
     ]
    }
   ],
   "source": [
    "encoded_X_test = model.encoder_predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['encoded_X_test.joblib']"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(encoded_X_test, \"encoded_X_test.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.12009786, 0.12640166, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.44631195, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.21731094, 0.4441418 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.1147465 ,\n",
       "       0.27347428, 0.        , 0.        , 0.36857966, 0.        ,\n",
       "       0.05280513, 0.36706027, 0.        , 0.37698156, 0.        ,\n",
       "       0.        , 0.        , 0.08929306, 0.1660731 , 0.        ,\n",
       "       0.        , 0.15715303, 0.3208198 , 0.        , 0.        ,\n",
       "       0.1448954 , 0.20213398, 0.5857546 , 0.14967312, 0.        ,\n",
       "       0.22405055, 0.        , 0.34649685, 0.5353485 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.48480624, 0.17144634, 0.23440376,\n",
       "       0.2678754 , 0.5993608 , 0.        , 0.01253927, 0.22427464,\n",
       "       0.        , 0.1107884 , 0.        , 0.        , 0.2952196 ,\n",
       "       0.        , 0.22177884, 0.        , 0.22270393, 0.28120175,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.36129546,\n",
       "       0.25969753, 0.09154522, 0.15598956, 0.        , 0.        ,\n",
       "       0.        , 0.2241052 , 0.        , 0.        , 0.13318375,\n",
       "       0.        , 0.44152647, 0.        , 0.        , 0.        ,\n",
       "       0.18175687, 0.19059871, 0.45567626, 0.        , 0.5305491 ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.16770424, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.23550186, 0.        , 0.10059728, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.2179561 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.3830049 , 0.        , 0.        ,\n",
       "       0.27061126, 0.02715881, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.58449876, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.23600909, 0.        , 0.33796668, 0.        ,\n",
       "       0.        , 0.72547835, 0.        , 0.72666705, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.2822798 ,\n",
       "       0.        , 0.        , 0.27690312, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.10973506,\n",
       "       0.44301122, 0.        , 0.        , 0.16510376, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.5159579 , 0.10633372, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.3039891 , 0.        , 0.        ,\n",
       "       0.05502365, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.24949047, 0.        , 0.        , 0.19425167, 0.        ,\n",
       "       0.        , 0.24322173, 0.21855035, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.31508783,\n",
       "       0.        , 0.        , 0.20271626, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.19645943, 0.        ,\n",
       "       0.09621526, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.6113442 , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.19425967,\n",
       "       0.        , 0.13149202, 0.        , 0.        , 0.24263188,\n",
       "       0.08870883, 0.        , 0.        , 0.        , 0.3877029 ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.26994723, 0.        , 0.        ,\n",
       "       0.24982461, 0.351472  , 0.        , 0.7919445 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.07246547, 0.        , 0.1033424 ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.40324312,\n",
       "       0.        , 0.229074  , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.44568175,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.29336712, 0.        , 0.        , 0.        , 0.25933462,\n",
       "       0.        , 0.        , 0.35524958, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.37563208,\n",
       "       0.        , 0.        , 0.4209407 , 0.        , 0.        ,\n",
       "       0.        , 0.21283033, 0.20310047, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.1051756 , 0.        ,\n",
       "       0.        , 0.        , 0.3962209 , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.23949674, 0.35678297, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.348291  , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.13017787, 0.        , 0.39090624, 0.37097958, 0.32473773,\n",
       "       0.3636572 , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.16707751, 0.        , 0.        , 0.43297523, 0.        ,\n",
       "       0.        , 0.        , 0.18739508, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.37651134, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.12151376,\n",
       "       0.        , 0.        , 0.        , 0.43353313, 0.24804696,\n",
       "       0.        , 0.30759665, 0.        , 0.        , 0.07881939,\n",
       "       0.        , 0.        , 0.        , 0.23548868, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.29246637, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.33948845, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.24864101, 0.3155122 , 0.        , 0.        , 0.        ,\n",
       "       0.4794255 , 0.        , 0.        , 0.        , 0.15477921,\n",
       "       0.        , 0.        , 0.18606743, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.10363199, 0.        ,\n",
       "       0.        , 0.        , 0.33174047, 0.12772897, 0.39009526,\n",
       "       0.        , 0.09227075, 0.35578132, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.3328569 , 0.10432659, 0.11288024,\n",
       "       0.        , 0.37344036, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.34872466,\n",
       "       0.3274348 , 0.        , 0.38819128, 0.        , 0.        ,\n",
       "       0.        , 0.34571576, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.32951593, 0.        , 0.        ,\n",
       "       0.        , 0.04946512, 0.314826  , 0.        , 0.2902245 ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_X_train[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300000, 500)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_X_train[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optuna "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300000,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros_like(Y_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KFoldsAverageMLP():\n",
    "    def __init__(self, FOLDS):\n",
    "        self.models = []\n",
    "        self.kfolds = KFold(n_splits = FOLDS, shuffle = False)\n",
    "        \n",
    "    def fit(self, trial, train_x, train_y, prune = True):\n",
    "        oof_preds = np.zeros_like(train_y)\n",
    "        self.train_x = train_x\n",
    "        self.train_y = train_y.values\n",
    "\n",
    "        \n",
    "        \n",
    "        # adding callbacks\n",
    "        model_save = ModelCheckpoint('./best_MLP_model.h5', \n",
    "                             save_best_only = True, \n",
    "                             save_weights_only = True,\n",
    "                             monitor = 'val_loss', \n",
    "                             mode = 'min', verbose = 10)\n",
    "        early_stop = EarlyStopping(monitor = 'val_loss', min_delta = 0.001, \n",
    "                           patience = 5, mode = 'min', verbose = 10,\n",
    "                           restore_best_weights = True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.1, \n",
    "                              patience = 3, min_delta = 0.001, \n",
    "                              mode = 'min', verbose = 10)\n",
    "        \n",
    "        ###############################################################\n",
    "        stack_num, num_data, encoded_features = self.train_x.shape\n",
    "        \n",
    "        # tunable hyperparameters\n",
    "        input_dense = int(trial.suggest_loguniform('input_dense_layer', 16, 1024))\n",
    "        input_dropout = trial.suggest_loguniform('input_dropout', 1e-10, 1)\n",
    "        num_layers =  trial.suggest_int('num_layers', 10, 40)\n",
    "        learning_rate = trial.suggest_categorical('learning_rate', [0.999, 1e-1, 5e-2, 1e-2, 1e-3])\n",
    "        \n",
    "        combined_dense = int(trial.suggest_loguniform('combined_dense_layer', 16, 1024))\n",
    "        combined_dropout = trial.suggest_loguniform('combined_dropout', 1e-10, 1)\n",
    "                    \n",
    "        # the 3 deepstack layers would go through a mini-model before concat to combined NN\n",
    "        input_a = keras.Input(shape = (encoded_features,), name = 'deepstack layer 1 input')\n",
    "        x = Dense(input_dense, activation = 'relu')(input_a)\n",
    "        x = Dropout(input_dropout)(x)\n",
    "        model_a = keras.Model(input_a, x, name = 'stack_1')\n",
    "        \n",
    "        input_b = keras.Input(shape = (encoded_features,), name = 'deepstack layer 2 input')\n",
    "        y = Dense(input_dense, activation = 'relu')(input_b)\n",
    "        y = Dropout(input_dropout)(y)\n",
    "        model_b = keras.Model(input_b,  y, name = 'stack_2')\n",
    "        \n",
    "        \n",
    "        input_c = keras.Input(shape = (encoded_features,), name = 'deepstack layer 3 input')\n",
    "        z = Dense(input_dense, activation = 'relu')(input_c)\n",
    "        z = Dropout(input_dropout)(z)\n",
    "        model_c = keras.Model(input_c,  z, name = 'stack_3')\n",
    "        \n",
    "        # the output from the three mini models \n",
    "        merged = keras.layers.Concatenate(axis=1)([x, y, z])\n",
    "        combined = Dense(combined_dense, activation = 'relu', name = 'combining_dense')(merged)\n",
    "        piped_data = Dropout(combined_dropout)(combined)\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            num_hidden = int(trial.suggest_loguniform(f'n_units_l{i}', 32, 1024))\n",
    "            dropout_rate = trial.suggest_loguniform(f'dropout_rate{i}', 1e-10, 1)\n",
    "            piped_data = Dense(num_hidden, activation='relu')(piped_data)\n",
    "            piped_data = Dropout(rate=dropout_rate)(piped_data)\n",
    "\n",
    "        output = Dense(units=1, activation = 'linear')(piped_data)\n",
    "        MLP = keras.Model([input_a, input_b, input_c], output)\n",
    "        \n",
    "        # Tune the learning rate for the optimizer \n",
    "        # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
    "        MLP.compile(optimizer = Adam(learning_rate = learning_rate), loss = 'mse', metrics=[keras.metrics.RootMeanSquaredError()])\n",
    "    \n",
    "        \n",
    "        for train_idx, val_idx in self.kfolds.split(train_x[0]):\n",
    "            \n",
    "            \n",
    "            # spliting the three deepstack layers into different input models before combining \n",
    "            X_train_CV0, X_val_CV0 = self.train_x[0][train_idx], self.train_x[0][val_idx]\n",
    "            X_train_CV1, X_val_CV1 = self.train_x[1][train_idx], self.train_x[1][val_idx]\n",
    "            X_train_CV2, X_val_CV2 = self.train_x[2][train_idx], self.train_x[2][val_idx]\n",
    "            \n",
    "            Y_train_CV, Y_val_CV = self.train_y[train_idx], self.train_y[val_idx]\n",
    "            \n",
    "            MLP.fit(x = [X_train_CV0, X_train_CV1, X_train_CV2], \n",
    "                      y = Y_train_CV, \n",
    "                      epochs = 1000,\n",
    "                      verbose = 1, \n",
    "                      validation_data = ([X_val_CV0, X_val_CV1, X_val_CV2] , Y_val_CV),\n",
    "                      callbacks = [early_stop,\n",
    "                                    reduce_lr])       \n",
    "        \n",
    "            self.models.append(MLP)\n",
    "            oof_pred = MLP.predict([X_val_CV0, X_val_CV1, X_val_CV2])\n",
    "            oof_preds[val_idx] = oof_pred[0]\n",
    "            \n",
    "        self.oof_preds = oof_preds\n",
    "        \n",
    "        self.rmse = mean_squared_error(Y_train, oof_preds, squared = False)\n",
    "\n",
    "    def predict(self, test_x):\n",
    "        preds = []\n",
    "        for model in tqdm.tqdm(self.models):\n",
    "            pred = model.predict(test_x)\n",
    "            preds.append(pred)\n",
    "        preds = np.mean(preds, axis=0)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_keras(trial):   \n",
    "    optuna_MLP = KFoldsAverageMLP(FOLDS = 5)\n",
    "    optuna_MLP.fit(trial = trial, train_x = encoded_X_train, train_y = Y_train, prune = True)\n",
    "    return optuna_MLP.rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-02-18 21:37:20,412]\u001b[0m A new study created in memory with name: no-name-2af4b53a-b32a-48f6-8942-5d2829fc04bb\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "7500/7500 [==============================] - 164s 22ms/step - loss: 555.7933 - root_mean_squared_error: 17.9228 - val_loss: 0.7850 - val_root_mean_squared_error: 0.8860\n",
      "Epoch 2/1000\n",
      "7500/7500 [==============================] - 158s 21ms/step - loss: 0.9239 - root_mean_squared_error: 0.9610 - val_loss: 0.7904 - val_root_mean_squared_error: 0.8891\n",
      "Epoch 3/1000\n",
      "7500/7500 [==============================] - 153s 20ms/step - loss: 0.7942 - root_mean_squared_error: 0.8912 - val_loss: 0.7839 - val_root_mean_squared_error: 0.8854\n",
      "Epoch 4/1000\n",
      "7500/7500 [==============================] - 153s 20ms/step - loss: 0.7886 - root_mean_squared_error: 0.8880 - val_loss: 0.7842 - val_root_mean_squared_error: 0.8855\n",
      "Epoch 5/1000\n",
      "7500/7500 [==============================] - 153s 20ms/step - loss: 0.7923 - root_mean_squared_error: 0.8901 - val_loss: 0.7843 - val_root_mean_squared_error: 0.8856\n",
      "Epoch 6/1000\n",
      "7500/7500 [==============================] - 157s 21ms/step - loss: 0.7881 - root_mean_squared_error: 0.8877 - val_loss: 0.7857 - val_root_mean_squared_error: 0.8864\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 7/1000\n",
      "7500/7500 [==============================] - 157s 21ms/step - loss: 0.7868 - root_mean_squared_error: 0.8870 - val_loss: 0.7840 - val_root_mean_squared_error: 0.8854\n",
      "Epoch 8/1000\n",
      "7500/7500 [==============================] - 160s 21ms/step - loss: 0.7896 - root_mean_squared_error: 0.8886 - val_loss: 0.7839 - val_root_mean_squared_error: 0.8854\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "Epoch 1/1000\n",
      "7500/7500 [==============================] - 163s 22ms/step - loss: 0.7869 - root_mean_squared_error: 0.8871 - val_loss: 0.7892 - val_root_mean_squared_error: 0.8884\n",
      "Epoch 2/1000\n",
      "7500/7500 [==============================] - 161s 21ms/step - loss: 0.7869 - root_mean_squared_error: 0.8871 - val_loss: 0.7891 - val_root_mean_squared_error: 0.8883\n",
      "Epoch 3/1000\n",
      "7500/7500 [==============================] - 162s 22ms/step - loss: 0.7869 - root_mean_squared_error: 0.8871 - val_loss: 0.7892 - val_root_mean_squared_error: 0.8884\n",
      "Epoch 4/1000\n",
      "2928/7500 [==========>...................] - ETA: 1:33 - loss: 0.7857 - root_mean_squared_error: 0.8864"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-dd319833e80e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmlp_study\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"minimize\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpruner\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpruners\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHyperbandPruner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmlp_study\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjective_keras\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcb_study\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\optuna\\study.py\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    379\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m             \u001b[0mgc_after_trial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgc_after_trial\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 381\u001b[1;33m             \u001b[0mshow_progress_bar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshow_progress_bar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    382\u001b[0m         )\n\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\optuna\\_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     68\u001b[0m                 \u001b[0mreseed_sampler_rng\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m                 \u001b[0mtime_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m                 \u001b[0mprogress_bar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m             )\n\u001b[0;32m     72\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\optuna\\_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m             \u001b[0mtrial\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m             \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\optuna\\_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 198\u001b[1;33m         \u001b[0mvalue_or_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m         \u001b[1;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-47-5cc32aa5eaca>\u001b[0m in \u001b[0;36mobjective_keras\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mobjective_keras\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0moptuna_MLP\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKFoldsAverageMLP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFOLDS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0moptuna_MLP\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoded_X_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprune\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0moptuna_MLP\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrmse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-46-399423fd83e2>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, trial, train_x, train_y, prune)\u001b[0m\n\u001b[0;32m     88\u001b[0m                       \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX_val_CV0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val_CV1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val_CV2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mY_val_CV\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m                       callbacks = [early_stop,\n\u001b[1;32m---> 90\u001b[1;33m                                     reduce_lr])       \n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMLP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    853\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 2943\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2945\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1919\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    561\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mlp_study = optuna.create_study(direction=\"minimize\", pruner = optuna.pruners.HyperbandPruner())\n",
    "mlp_study.optimize(objective_keras, n_trials=50)\n",
    "print(cb_study.best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
